# 分布式系统设计

## 服务注册发现（CAP理论下Eureka与Nacos对比）
    CAP理论,C一致性（Consistency），A 可用性（Availability），P分区容错性（Partition Tolerance）,理论上这三点无法同时满足，
    C 一致性：所有节点在同一时刻看到的数据完全相同，即强一致性。
    A 可用性：系统在任何情况下均能响应请求（非错误或超时），即使部分节点故障。例如，社交媒体应用需保证用户随时刷新动态
    P 分区容错性: 系统在网络分区（节点间通信中断）时仍能继续运行。例如，跨地域数据中心需容忍网络波动
- 三者不可兼得的根本原因
  - 选择CP：若保证一致性，需等待分区节点数据同步完成才能响应请求，导致可用性下降。例如，ZooKeeper在选举Leader期间会暂停服务
  - 选择AP：若保证可用性，允许节点返回旧数据，牺牲一致性。例如，Eureka在网络分区时仍接受服务注册，但不同节点数据可能短暂不一致
  - CA场景的局限性：仅在单机或无网络分区的理想环境下成立，无法满足分布式系统的现实需求
- 典型应用场景与系统举例
  - CP系统
    - 金融交易系统（如银行核心系统）：需强一致性，即使停机维护也要保证转账金额准确
    - 分布式协调工具（如ZooKeeper、etcd）：通过Raft协议实现强一致性，用于Kubernetes元数据管理等场景
  - AP系统：
    - 高并发服务发现（如Eureka、Nacos AP模式）：容忍短暂不一致，优先保障服务注册与发现的可用性
    - 缓存与消息队列（如Redis集群、RabbitMQ）：通过最终一致性支持高吞吐量
  - AC系统
    - 单机数据库（如MySQL单实例）：无网络分区问题，但无法扩展为分布式架构
- Eureka 严格遵循AP模式 优先保证可用性（A）和分区容错性（P）
  - 在网络分区（如节点宕机或网络中断）时，Eureka会进入自我保护模式，保留所有注册信息以避免服务不可用，但可能导致不同节点间的数据短暂不一致
  - 采用内存存储，无持久化机制，依赖客户端定时上报心跳（默认30秒），服务列表通过定时拉取（默认30秒）更新，存在秒级延迟
  - 通过增量数据同步减少网络开销，但节点间数据最终一致需依赖异步复制
  - 节点宕机后，其他节点仍可接受服务注册与查询，但新节点加入需手动同步数据，恢复时间较长
  - 适合高可用性优先的云原生场景，如电商、社交等对短暂数据不一致容忍度较高的业务
  - Eureka仅支持HTTP心跳检测
  - Eureka提供区域（Region/Zone）两级隔离
- Nacos 支持AP和CP
  - AP模式：通过Distro协议实现最终一致性，适用于临时实例注册（需客户端心跳续约），容忍短暂数据不一致以保障高可用
    - AP模式下：数据分片存储，节点间异步复制，支持服务列表变更的主动推送（Push模式），更新时效性更高
    - AP模式：Leader节点宕机时，秒级自动选举新Leader，服务注册不受影响
    - AP模式：适用于大多数微服务场景，如互联网应用、实时性要求较高的服务发现
  - CP模式：通过Raft协议保证强一致性，适用于持久化实例或配置中心场景，确保数据实时一致，但牺牲部分可用性（如Leader选举期间服务注册阻塞）
    - CP模式下：基于Raft协议实现强一致性，所有写操作需经Leader节点同步到多数派节点，保证数据实时一致
    - CP模式：Leader选举期间（约3-10秒），服务注册短暂阻塞，客户端通过重试机制自动恢复
    - CP模式：用于金融交易、配置管理等强一致性要求的场景
  - Nacos支持TCP主动探测（非临时实例）和HTTP心跳，提供更精准的健康状态判断
  - Nacos支持Namespace、Group、Service三级隔离，细粒度更高
- Eureka以简单、高可用见长，适合传统Spring Cloud生态；而Nacos凭借模式切换、实时推送和强一致性能力，更适合复杂业务场景（如混合云、动态配置管理）。实际选型需权衡业务对一致性、可用性及运维成本的需求

## 熔断降级策略（Hystrix vs Sentinel）
- Hystrix
  - 核心目标：专注于容错与故障隔离，通过熔断、线程池隔离、降级等手段阻止级联故障
  - 实现方式：基于命令模式（HystrixCommand）封装请求，每个资源独立线程池或信号量隔离，牺牲部分性能换取强隔离性
  - 局限性：2018年后停止更新，配置复杂且扩展性较差，适用于传统Spring Cloud体系
  - Hystrix适用场景
    - 传统微服务架构：需强隔离性（如金融交易系统）
    - 存量系统维护：已有Hystrix集成且无性能瓶颈的场景
  - 选择Hystrix的情况
    - 现有Spring Cloud Netflix体系且无性能瓶颈。
    - 需要严格的线程池隔离（如资源敏感型服务）

- Sentinel
  - 核心目标：以流量控制为核心，扩展至熔断降级、系统负载保护等场景，支持动态规则配置与多语言生态
  - 实现方式：轻量级非阻塞设计，通过资源埋点（注解或API）统计实时指标，支持QPS、响应时间、热点参数等多维度控制
  - 优势：持续更新，与Spring Cloud Alibaba深度集成，适用于云原生环境和高性能场景
  - Sentinel适用场景
    - 高并发互联网应用：如电商秒杀、支付系统（如双11支持12万QPS）
    - 动态规则需求：需实时调整阈值或灰度发布
  - 优先选择Sentinel的情况
    - 高吞吐量需求（如10万+ QPS）
    - 需灵活的动态规则管理（如双11实时调优）
    - 多语言支持或云原生环境（如Kubernetes）
## 分布式事务
  - 2PC（两阶段提交）
    - 1阶段：事务协调者询问事务参与者是否准备好，执行但不提交
    - 2阶段：通知所有的事务参与者提交
    - 异常情况
      - 1阶段异常：某个事务参与者在1阶段出现异常/协调者没有收到参与者的确认，协调者在2阶段通知所有参与者全部回滚
      - 2阶段异常：协调者发送commit后，某个参与者未收到消息/失败，协调者多次重试再次发送提交请求
  - 3PC（三阶段提交）
    - 1阶段：协调者向所有参与者发送询问请求，确认参与者的状态(网络状态，服务是否可用)
    - 2阶段：若所有参与者返回确认后，向各个参与者发送preCommit请求，要求参与者执行事务但不提交
    - 3阶段：若所有参与者返回确认后，向各个参与者发送提交请求，
    - 异常情况：
      - 1阶段异常：某个参与者在1阶段返回错误或超时，向所有参与者发送终止请求
      - 2阶段异常：某个事务在2阶段返回错误或异常时，向所有参与者发送回滚请求
      - 3阶段异常：协调者宕机，则参与者默认提交事务，参与者宕机则协调者重新发送提交请求
    - 相比2阶段提交的优点
      - 减少同步阻塞：2阶段提交，如果协调者发送1阶段请求后宕机，参与者会无限阻塞
      - 缓解单点故障风险：2阶段提交，参与者故障，其他参与者仍然会执行操作，浪费性能，协调者故障，事务无法恢复，3阶段中协调者和参与者都有主动推进事务的能力
      - 降低数据不一致的概率：2阶段协调者故障，有可能导致部分参与者提交而部分参与者不提交，导致不一致，3阶段协调者如果宕机，参与者默认提交
  - TCC（Try-Confirm-Cancel）
    - 1阶段（试探阶段）：协调者向参与者发起资源预留请求，参与者执行本地事务操作（如写入冻结记录），但不提交最终事务。
    - 2阶段（确认阶段）：协调者向所有参与者发送 Confirm 指令。参与者将预留资源正式生效（如扣减冻结资金、减少库存数量）。
    - 2阶段（取消阶段）：协调者向所有参与者发送取消指令，取消1阶段的数据信息
    - 默认2阶段一定成功，如果2阶段失败，需要定时检测和人工
  - SAGA
    - 核心思想，长事务拆分成多个本地的短事务，如果事务执行中出现错误，会撤销之前所有成功的子事务
  - MQ事务消息
    - 核心思想是通过半消息机制和事务状态回查实现消息与本地事务的协同，
    - 半消息机制
      - 半消息是对消费者不可见的中间状态的消息，仅在本地事务成功后才转为可投递状态
    - 二阶段提交
      - 1阶段：生产者发送半消息到MQ，MQ持久化后返回ACK确认
      - 2阶段：生产者执行本地事务，根据结果向MQ发送COMMIT或ROLLBACK消息
## Seata AT模式（全局锁冲突处理）
  - AT模式 过程
    - 开启全局事务、注册分支事务、储存全局锁、业务数据和回滚日志提交
    - 事务协调者根据所有分支的情况，决定本次全局事务是commit还是rollback
## TCC模式空回滚问题解决方案

## 高并发设计

## 限流算法（令牌桶 vs 漏桶实际应用场景）

## 分库分表（ShardingSphere基因法分片）

## 分库分表后全局ID生成（Snowflake时钟回拨问题）

## 热点数据缓存（本地缓存+Redis多级缓存）


